{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Production\n",
    "So the approach that I used (for simplicity) that I get the data from dataframe. based on a real world scenario it could be upload or pulled from or some other services then we can change the code accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the model\n",
    "loaded_model = joblib.load('../models/random_forest_model.pkl')\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "# predictions = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "     \"\"\"\n",
    "     Extracts features from raw sensor data at the session level.\n",
    "     \n",
    "     Parameters:\n",
    "          df (pd.DataFrame): The sensor data containing columns ['id', 'time', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'side']\n",
    "          time could come also as index for the dataframe as well\n",
    "     \n",
    "     Returns:\n",
    "          pd.DataFrame: Processed session-level features with step counts.\n",
    "     \"\"\"\n",
    "     # Ensure 'time' is a datetime type\n",
    "     if df.get(\"time\") is not None:\n",
    "          if not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "               df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "          df.set_index('time', inplace=True)\n",
    "          df = df.sort_index()\n",
    "          \n",
    "     if not isinstance(df.index, pd.DatetimeIndex):\n",
    "          df.index = pd.to_datetime(df.index)\n",
    "     \n",
    "\n",
    "\n",
    "     # Extract time-based features\n",
    "     session_metadata[\"session_duration\"] = (df.index.max() - df.index.min()).total_seconds()\n",
    "     session_metadata['num_measurements'] = len(df)\n",
    "     session_metadata['start_time'] = df.index.min()\n",
    "     session_metadata['end_time'] = df.index.max()\n",
    "     # maybe not need\n",
    "     # if df.get(\"time_diff\") is None:\n",
    "     #      df[\"time_diff\"] = df.index.to_series().diff().dt.total_seconds()\n",
    "     \n",
    "     \n",
    "     # Aggregate sensor statistics per session\n",
    "     session_features = df.groupby(\"id\").agg({\n",
    "          \"ax\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "          \"ay\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "          \"az\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "          \"gx\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "          \"gy\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "          \"gz\": [\"mean\", \"std\", \"min\", \"max\"]\n",
    "          # \"time_diff\": [\"sum\", \"count\"]  # Sum gives session duration\n",
    "     }).reset_index()\n",
    "\n",
    "     # Flatten MultiIndex columns\n",
    "     session_features.columns = [\"_\".join(col).strip() for col in session_features.columns.values if col != \"id\"]\n",
    "     # session_features = session_features.rename(columns={\"id_\": \"id\"})\n",
    "\n",
    "     # Function to count steps using peaks in 'az'\n",
    "     def count_peaks(series):\n",
    "          peaks, _ = find_peaks(series, height=-1.66)  \n",
    "          return len(peaks)\n",
    "\n",
    "     step_counts = df.groupby([\"id\", \"side\"])[\"az\"].apply(count_peaks).unstack(fill_value=0)\n",
    "     step_counts.columns = [\"left_steps\", \"right_steps\"]\n",
    "\n",
    "     # merge all data\n",
    "     final_df = session_metadata.merge(session_features, on=\"id\").merge(step_counts, on=\"id\")\n",
    "\n",
    "     return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions for a New Session\n",
    "def predict_steps(new_session_data):\n",
    "    new_session_features = extract_features(new_session_data)  \n",
    "\n",
    "    # Extract only model input features\n",
    "    X_new = new_session_features.drop(columns=[\"id\", \"start_time\", \"end_time\", \"left_steps\", \"right_steps\"])\n",
    "\n",
    "    # Predict step counts\n",
    "    predicted_steps = model.predict(X_new)\n",
    "\n",
    "    # Convert predictions to a structured output\n",
    "    new_session_features[\"left_steps_pred\"] = predicted_steps[:, 0]\n",
    "    new_session_features[\"right_steps_pred\"] = predicted_steps[:, 1]\n",
    "\n",
    "    # Final structured output\n",
    "    output = new_session_features[[\"id\", \"start_time\", \"end_time\", \"left_steps_pred\", \"right_steps_pred\"]]\n",
    "    output.rename(columns={\"left_steps_pred\": \"left_steps\", \"right_steps_pred\": \"right_steps\"}, inplace=True)\n",
    "\n",
    "    return output.to_dict(orient=\"records\")\n",
    "\n",
    "# Example Prediction\n",
    "example_session = df[df[\"id\"] == \"0373xrf1eaJoc8IcE6Gc\"]\n",
    "predicted_steps = predict_steps(example_session)\n",
    "print(f\"Predicted Steps: {predicted_steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
